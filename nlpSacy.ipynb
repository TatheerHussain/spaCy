{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlpSacy.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMRRU4+Pt4T8BIr8MAW+YJq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TatheerHussain/spaCy/blob/master/nlpSacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SO36Rx7L6F6E",
        "colab_type": "text"
      },
      "source": [
        "# This collab Notbook is about using Natural Language Processing (NLP),\n",
        "\n",
        "> We will use the leading NLP library (spaCy) to take on some of the most important tasks in working with text.\n",
        "\n",
        "\n",
        "> This Notebook will hel you to get Understanding of nlp and spacy and \n",
        "By the end of this notebook, you will be able to use spaCy for:\n",
        "\n",
        "\n",
        "*   Basic text processing and pattern matching\n",
        "*   Building machine learning models with text\n",
        "*   Representing text with word embeddings that numerically capture the meaning of words and documents "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_89HtXX_l4s",
        "colab_type": "text"
      },
      "source": [
        "# NLP with spaCy\n",
        "#### spaCy is the leading library for NLP, and it has quickly become one of the most popular Python frameworks. Most people find it intuitive, and it has excellent documentation.\n",
        "\n",
        "> spaCy relies on models that are language-specific and come in different sizes. You can load a spaCy model with \n",
        "```\n",
        "spacy.load\n",
        "```\n",
        "For example, here's how you would load the English language model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9hXHDBx0Onu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pip install -U spaCy\n",
        "#python -m spacy download en\n",
        "\n",
        "\n",
        "# Notice that the installation doesn’t automatically download the English model. We need to do that ourselves."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArsoWL-L6ima",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfbUufzW0iWx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "84826ba6-bff9-4db8-e5ec-22a541343461"
      },
      "source": [
        "doc = nlp('Hello     World!')\n",
        "for token in doc:\n",
        "    print('\"' + token.text + '\"')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"Hello\"\n",
            "\"    \"\n",
            "\"World\"\n",
            "\"!\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdK1jZng1jpb",
        "colab_type": "text"
      },
      "source": [
        "Notice the index preserving tokenization in action in the above example.\n",
        "Rather than only keeping the words, spaCy keeps the spaces too. as we can see it clearly after hello being printed.\n",
        "This is helpful for situations when you need to replace words in the original text or add some annotations. \n",
        "With NLTK tokenization, there’s no way to know exactly where a tokenized word is in the original raw text. spaCy preserves this “link” between the word and its place in the raw text. Here’s how to get the exact index of a word and psaces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pI0V6li2AsM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "80be649c-d438-487d-96d4-83acbdb7cd89"
      },
      "source": [
        "doc = nlp('Hello     World!')\n",
        "for token in doc:\n",
        "    print('\"' + token.text + '\"', token.idx)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"Hello\" 0\n",
            "\"    \" 6\n",
            "\"World\" 10\n",
            "\"!\" 15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHpaYJWe6dDq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "21a3e5a3-59d7-4434-a03e-1d6324e5e4f8"
      },
      "source": [
        "doc = nlp(\"These are apples. These are oranges.\")\n",
        " \n",
        "for sent in doc.sents:\n",
        "    print(sent)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "These are apples.\n",
            "These are oranges.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtzoLrQ121di",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f17b0c3c-ea00-4733-f881-5a43348145a7"
      },
      "source": [
        "doc = nlp(\"Next week I'll   be in Kashmir.\")\n",
        "print(f\"Text \\t\\tindex \\t\\t lemma \\t\\t punctuation \\t space \\t\\t shape \\t\\t pos \\t\\t tag\".format('Text','index','lemma','punctuation','space','shape','pos','tag'))\n",
        "print(\"______________________________________________________________________________________________________________________________\")\n",
        "for token in doc:\n",
        "    print(\"{0}\\t\\t{1}\\t\\t{2}\\t\\t{3}\\t\\t{4}\\t\\t{5}\\t\\t{6}\\t\\t{7}\".format(\n",
        "        token.text,\n",
        "        token.idx,\n",
        "        token.lemma_,\n",
        "        token.is_punct,\n",
        "        token.is_space,\n",
        "        token.shape_,\n",
        "        token.pos_,\n",
        "        token.tag_\n",
        "    ))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text \t\tindex \t\t lemma \t\t punctuation \t space \t\t shape \t\t pos \t\t tag\n",
            "______________________________________________________________________________________________________________________________\n",
            "Next\t\t0\t\tnext\t\tFalse\t\tFalse\t\tXxxx\t\tADJ\t\tJJ\n",
            "week\t\t5\t\tweek\t\tFalse\t\tFalse\t\txxxx\t\tNOUN\t\tNN\n",
            "I\t\t10\t\t-PRON-\t\tFalse\t\tFalse\t\tX\t\tPRON\t\tPRP\n",
            "'ll\t\t11\t\twill\t\tFalse\t\tFalse\t\t'xx\t\tVERB\t\tMD\n",
            "  \t\t15\t\t  \t\tFalse\t\tTrue\t\t  \t\tSPACE\t\t_SP\n",
            "be\t\t17\t\tbe\t\tFalse\t\tFalse\t\txx\t\tAUX\t\tVB\n",
            "in\t\t20\t\tin\t\tFalse\t\tFalse\t\txx\t\tADP\t\tIN\n",
            "Kashmir\t\t23\t\tKashmir\t\tFalse\t\tFalse\t\tXxxxx\t\tPROPN\t\tNNP\n",
            ".\t\t30\t\t.\t\tTrue\t\tFalse\t\t.\t\tPUNCT\t\t.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpZUEnQVAZ0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we will try now with more set of examples \n",
        "\n",
        "docs = nlp(\"Tea makes your mind fresh, healthy and calming, haven't you expereinced so?\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8wvsHlDuE0A",
        "colab_type": "text"
      },
      "source": [
        "# Why spaCy\n",
        "\n",
        "**It's really FAST**\n",
        "\n",
        "Written in Cython, it was specifically designed to be as fast as possible\n",
        "\n",
        "**It's really ACCURATE**\n",
        "\n",
        "spaCy implementation of its dependency parser is one of the best-performing in the world:\n",
        "It Depends: Dependency Parser Comparison\n",
        "Using A Web-based Evaluation Tool\n",
        "Batteries included\n",
        "Index preserving tokenization (details about this later)\n",
        "Models for Part Of Speech tagging, Named Entity Recognition and Dependency Parsing\n",
        "Supports 8 languages out of the box\n",
        "Easy and beautiful visualizations\n",
        "Pretrained word vectors\n",
        "Extensible\n",
        "It plays nicely with all the other already existing tools that you know and love: Scikit-Learn, TensorFlow, gensim\n",
        "DeepLearning Ready\n",
        "It also has its own deep learning framework that’s especially designed for NLP tasks:\n",
        "Thinc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nai3MCpgBgey",
        "colab_type": "text"
      },
      "source": [
        "**Now we will go further ahead to explore what we can do with the \"docs\" object we just created above**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFtEJ2HhBpoG",
        "colab_type": "text"
      },
      "source": [
        "# Tokenizing\n",
        "> A token is a unit of text in the document, such as individual words and punctuation. or in simpler terms Tokenizing means splitting your text into minimal meaningful units. It is a mandatory step before any kind of processing. SpaCy splits contractions like \"don't\" into two tokens, \"do\" and \"n't\". You can see the tokens by iterating through the document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8J-1SVyChw0",
        "colab_type": "code",
        "outputId": "e99f9f10-2f45-42f0-ef11-b18c605860bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "for token in docs:\n",
        "    print(token)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tea\n",
            "makes\n",
            "your\n",
            "mind\n",
            "fresh\n",
            ",\n",
            "healthy\n",
            "and\n",
            "calming\n",
            ",\n",
            "have\n",
            "n't\n",
            "you\n",
            "expereinced\n",
            "so\n",
            "?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqLThaBvDCKd",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWdDEmpuH-kv",
        "colab_type": "text"
      },
      "source": [
        "# Text preprocessing\n",
        "\n",
        "There are a few types of preprocessing to improve how we model with words. The first is \"lemmatizing.\" The \"lemma\" of a word is its base form. \n",
        "For example, \n",
        "\"walk\" is the lemma of the word \"walking\". So, when you lemmatize the word walking, you would convert it to walk.\n",
        "\n",
        "It's also common to remove stopwords. Stopwords are words that occur frequently in the language and don't contain much information. English stopwords include \"the\", \"is\", \"and\", \"but\", \"not\".\n",
        "\n",
        "With a spaCy token, token.lemma_ returns the lemma, while token.is_stop returns a boolean True if the token is a stopword (and False otherwise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6e68azcG7Gl",
        "colab_type": "code",
        "outputId": "8f2cf5e5-23a0-4456-bb33-7201046efadf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "print(f\"Token \\t\\t\\t Lemma \\t\\t\\t Stopword\".format('Token', 'Lemma', 'Stopword'))\n",
        "print(\"----------------------------------------\")\n",
        "for token in docs:\n",
        "    print(f\"{str(token)}\\t\\t\\t{token.lemma_}\\t\\t\\t{token.is_stop}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token \t\t\t Lemma \t\t\t Stopword\n",
            "----------------------------------------\n",
            "Tea\t\t\ttea\t\t\tFalse\n",
            "makes\t\t\tmake\t\t\tFalse\n",
            "your\t\t\t-PRON-\t\t\tTrue\n",
            "mind\t\t\tmind\t\t\tFalse\n",
            "fresh\t\t\tfresh\t\t\tFalse\n",
            ",\t\t\t,\t\t\tFalse\n",
            "healthy\t\t\thealthy\t\t\tFalse\n",
            "and\t\t\tand\t\t\tTrue\n",
            "calming\t\t\tcalming\t\t\tFalse\n",
            ",\t\t\t,\t\t\tFalse\n",
            "have\t\t\thave\t\t\tTrue\n",
            "n't\t\t\tnot\t\t\tTrue\n",
            "you\t\t\t-PRON-\t\t\tTrue\n",
            "expereinced\t\t\texpereince\t\t\tFalse\n",
            "so\t\t\tso\t\t\tTrue\n",
            "?\t\t\t?\t\t\tFalse\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK0-9CcLJuQK",
        "colab_type": "text"
      },
      "source": [
        "**Here we will mention that why are lemmas and identifying stopwords important? **\n",
        "\n",
        "Language data has a lot of noise mixed in with informative content. In the sentence above, the important words are tea, healthy, calming and exerienced.\n",
        "\n",
        " Removing **stop words** might help the predictive model focus on relevant words. \n",
        "**Lemmatizing** similarly helps by combining multiple forms of the same word into one base form (\"calming\", \"calms\", \"calmed\" would all change to \"calm\").\n",
        "\n",
        "However, lemmatizing and dropping stopwords might result in your models performing worse. So you should treat this preprocessing as part of your hyperparameter optimization process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j6HwS7YKdFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc52mpjkK0zI",
        "colab_type": "text"
      },
      "source": [
        "# Pattern Matching\n",
        "Another common NLP task is matching tokens or phrases within chunks of text or whole documents. You can do pattern matching with regular expressions, but spaCy's matching capabilities tend to be easier to use.\n",
        "\n",
        "To match individual tokens, you create a Matcher. When you want to match a list of terms, it's easier and more efficient to use PhraseMatcher. For example, if you want to find where different smartphone models show up in some text, you can create patterns for the model names of interest. First you create the PhraseMatcher itself"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mENR_OjnLFba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spacy.matcher import PhraseMatcher\n",
        "matcher = PhraseMatcher(nlp.vocab, attr='LOWER')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKiqvcrVLyGr",
        "colab_type": "text"
      },
      "source": [
        "The matcher is created using the vocabulary of your model. Here we're using the small English model you loaded earlier. Setting attr='LOWER' will match the phrases on lowercased text. This provides case insensitive matching.\n",
        "\n",
        "Next you create a list of terms to match in the text. The phrase matcher needs the patterns as document objects. The easiest way to get these is with a list comprehension using the nlp model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KruAUOG2LJej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "terms = ['Galaxy Note', 'iPhone 11', 'iPhone XS', 'Google Pixel']\n",
        "patterns = [nlp(text) for text in terms]\n",
        "matcher.add(\"TerminologyList\", None, *patterns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohgd5SM5PYGh",
        "colab_type": "text"
      },
      "source": [
        "Then we create a document from the text to search and use the phrase matcher to find where the terms occur in the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWCyeH8MPWra",
        "colab_type": "code",
        "outputId": "a5bed028-ea8e-4ea0-cf2f-870bc95eead5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text_doc = nlp(\"Glowing review overall, and some really interesting side-by-side \"\n",
        "               \"photography tests pitting the iPhone 11 Pro against the \"\n",
        "               \"Galaxy Note 10 Plus and last year’s iPhone XS and Google Pixel 3.\") \n",
        "matches = matcher(text_doc)\n",
        "print(matches)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(3766102292120407359, 17, 19), (3766102292120407359, 22, 24), (3766102292120407359, 30, 32), (3766102292120407359, 33, 35)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyBMDGOcPryB",
        "colab_type": "text"
      },
      "source": [
        "The matches here are a tuple of the match id and the positions of the start and end of the phrase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htlHgBcpPjEM",
        "colab_type": "code",
        "outputId": "0eda0b45-182d-49c3-b0e5-0694861f63a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "match_id, start, end = matches[0]\n",
        "print(nlp.vocab.strings[match_id], text_doc[start:end])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TerminologyList iPhone 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvpJLKQQPxk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyR-FkevRxnQ",
        "colab_type": "text"
      },
      "source": [
        "To understand better we will followup with an example\n",
        "\n",
        "**Basic Text processing with Spacy**\n",
        "\n",
        "Suppose you are a consultant for DelFalco's Italian Restaurant. The owner asked you to identify whether there are any foods on their menu that diners find disappointing.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fOMl0_WSp3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4suvbzPq7Xnz",
        "colab_type": "text"
      },
      "source": [
        "# Part Of Speech Tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4rM0Ef2S4ZI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dadb5227-933f-4fd6-f3a0-564a65bbe246"
      },
      "source": [
        "\n",
        "# now lets have look another look.\n",
        "doc = nlp(\"Next week I'll be in Kashmir.\")\n",
        "print([(token.text, token.tag_) for token in doc])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Next', 'JJ'), ('week', 'NN'), ('I', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('in', 'IN'), ('Kashmir', 'NNP'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww6bLgs871_s",
        "colab_type": "text"
      },
      "source": [
        "# Named Entity Recognition\n",
        "Doing NER with spaCy is easy and the pretrained model performs also well:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRdeVbAZ8MM8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f6addc06-bb60-4e79-e53b-e12051dd095f"
      },
      "source": [
        "doc = nlp(\"Next week i'll be in Kashmir\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Next week DATE\n",
            "Kashmir LOC\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh76zcC391yc",
        "colab_type": "text"
      },
      "source": [
        "**IOB style tagging of the sentence like this:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2flkMrf19DD6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ac0c1e94-69c2-4cd5-9f7c-272a2d014a1c"
      },
      "source": [
        "from nltk.chunk import conlltags2tree\n",
        " \n",
        "doc = nlp(\"Next week I'll be in Kashmir.\")\n",
        "iob_tagged = [\n",
        "    (\n",
        "        token.text, \n",
        "        token.tag_, \n",
        "        \"{0}-{1}\".format(token.ent_iob_, token.ent_type_) if token.ent_iob_ != 'O' else token.ent_iob_\n",
        "    ) for token in doc\n",
        "]\n",
        " \n",
        "print(iob_tagged)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Next', 'JJ', 'B-DATE'), ('week', 'NN', 'I-DATE'), ('I', 'PRP', 'O'), (\"'ll\", 'MD', 'O'), ('be', 'VB', 'O'), ('in', 'IN', 'O'), ('Kashmir', 'NNP', 'B-LOC'), ('.', '.', 'O')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-ncRqyj9WrB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "ab45a952-2654-41b0-fb21-a2416ac70008"
      },
      "source": [
        "# the same above results in nltk.Tree format\n",
        "print(conlltags2tree(iob_tagged))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (DATE Next/JJ week/NN)\n",
            "  I/PRP\n",
            "  'll/MD\n",
            "  be/VB\n",
            "  in/IN\n",
            "  (LOC Kashmir/NNP)\n",
            "  ./.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfpixN51sNa_",
        "colab_type": "text"
      },
      "source": [
        "The spaCy NER has a healthy variety of entities. if you want to read more about them follow this link:\n",
        "spaCy NER "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZVYoGnWAmtk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "2241e809-2994-4e87-8346-9afff5bc417f"
      },
      "source": [
        "from spacy import displacy\n",
        "\n",
        "text = \"When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.\"\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "displacy.serve(doc, style=\"ent\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Using the 'ent' visualizer\n",
            "Serving on http://0.0.0.0:5000 ...\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzgb2OjBAntU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = nlp(\"I just bought 2 shares at 9 a.m. because the stock went up 30% in just 2 days according to the WSJ\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n",
        " \n",
        "# 2 CARDINAL\n",
        "# 9 a.m. TIME\n",
        "# 30% PERCENT\n",
        "# just 2 days DATE\n",
        "# WSJ ORG"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqY0dVkyCmsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}